{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import math\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    色泽  根蒂  敲声  纹理  脐部  触感     密度    含糖率  好瓜\n",
      "0   青绿  蜷缩  沉闷  稍糊  稍凹  硬滑  0.719  0.103   0\n",
      "1   乌黑  稍蜷  浊响  稍糊  稍凹  软粘  0.481  0.149   1\n",
      "2   浅白  蜷缩  浊响  清晰  凹陷  硬滑  0.556  0.215   1\n",
      "3   乌黑  稍蜷  浊响  清晰  稍凹  硬滑  0.437  0.211   1\n",
      "4   乌黑  稍蜷  浊响  清晰  稍凹  软粘  0.360  0.370   0\n",
      "5   乌黑  蜷缩  沉闷  清晰  凹陷  硬滑  0.774  0.376   1\n",
      "6   浅白  蜷缩  浊响  模糊  平坦  软粘  0.343  0.099   0\n",
      "7   乌黑  蜷缩  浊响  清晰  凹陷  硬滑  0.634  0.264   1\n",
      "8   浅白  稍蜷  沉闷  稍糊  凹陷  硬滑  0.657  0.198   0\n",
      "9   青绿  稍蜷  浊响  清晰  稍凹  软粘  0.403  0.237   1\n",
      "10  青绿  蜷缩  浊响  清晰  凹陷  硬滑  0.697  0.460   1\n",
      "11  青绿  稍蜷  浊响  稍糊  凹陷  硬滑  0.639  0.161   0\n",
      "12  浅白  蜷缩  浊响  模糊  平坦  硬滑  0.593  0.042   0\n",
      "13  青绿  蜷缩  沉闷  清晰  凹陷  硬滑  0.608  0.318   1\n",
      "14  青绿  硬挺  清脆  清晰  平坦  软粘  0.243  0.267   0\n",
      "15  乌黑  稍蜷  沉闷  稍糊  稍凹  硬滑  0.666  0.091   0\n",
      "16  浅白  硬挺  清脆  模糊  平坦  硬滑  0.245  0.057   0\n",
      "    色泽青绿  色泽乌黑  色泽浅白  根蒂蜷缩  根蒂稍蜷  根蒂硬挺  敲声沉闷  敲声浊响  敲声清脆  纹理稍糊  纹理清晰  纹理模糊  \\\n",
      "0      1     0     0     1     0     0     1     0     0     1     0     0   \n",
      "1      0     1     0     0     1     0     0     1     0     1     0     0   \n",
      "2      0     0     1     1     0     0     0     1     0     0     1     0   \n",
      "3      0     1     0     0     1     0     0     1     0     0     1     0   \n",
      "4      0     1     0     0     1     0     0     1     0     0     1     0   \n",
      "5      0     1     0     1     0     0     1     0     0     0     1     0   \n",
      "6      0     0     1     1     0     0     0     1     0     0     0     1   \n",
      "7      0     1     0     1     0     0     0     1     0     0     1     0   \n",
      "8      0     0     1     0     1     0     1     0     0     1     0     0   \n",
      "9      1     0     0     0     1     0     0     1     0     0     1     0   \n",
      "10     1     0     0     1     0     0     0     1     0     0     1     0   \n",
      "11     1     0     0     0     1     0     0     1     0     1     0     0   \n",
      "12     0     0     1     1     0     0     0     1     0     0     0     1   \n",
      "13     1     0     0     1     0     0     1     0     0     0     1     0   \n",
      "14     1     0     0     0     0     1     0     0     1     0     1     0   \n",
      "15     0     1     0     0     1     0     1     0     0     1     0     0   \n",
      "16     0     0     1     0     0     1     0     0     1     0     0     1   \n",
      "\n",
      "    脐部稍凹  脐部凹陷  脐部平坦  触感硬滑  触感软粘     密度    含糖率  好瓜  \n",
      "0      1     0     0     1     0  0.719  0.103   0  \n",
      "1      1     0     0     0     1  0.481  0.149   1  \n",
      "2      0     1     0     1     0  0.556  0.215   1  \n",
      "3      1     0     0     1     0  0.437  0.211   1  \n",
      "4      1     0     0     0     1  0.360  0.370   0  \n",
      "5      0     1     0     1     0  0.774  0.376   1  \n",
      "6      0     0     1     0     1  0.343  0.099   0  \n",
      "7      0     1     0     1     0  0.634  0.264   1  \n",
      "8      0     1     0     1     0  0.657  0.198   0  \n",
      "9      1     0     0     0     1  0.403  0.237   1  \n",
      "10     0     1     0     1     0  0.697  0.460   1  \n",
      "11     0     1     0     1     0  0.639  0.161   0  \n",
      "12     0     0     1     1     0  0.593  0.042   0  \n",
      "13     0     1     0     1     0  0.608  0.318   1  \n",
      "14     0     0     1     0     1  0.243  0.267   0  \n",
      "15     1     0     0     1     0  0.666  0.091   0  \n",
      "16     0     0     1     1     0  0.245  0.057   0  \n"
     ]
    }
   ],
   "source": [
    "# read excel and geenrate dataframe\n",
    "np.random.seed(3)\n",
    "raw_dataset = pd.DataFrame(pd.read_excel(r'dataset_watermelon3.xlsx'))\n",
    "raw_dataset = raw_dataset.sample(frac = 1).reset_index(drop=True) # shuffle\n",
    "raw_dataset.loc[raw_dataset['好瓜']=='是','好瓜']=1\n",
    "raw_dataset.loc[raw_dataset['好瓜']=='否','好瓜']=0\n",
    "print(raw_dataset)\n",
    "\n",
    "# one-hot encoding\n",
    "watermelon_dataset = pd.DataFrame()\n",
    "for col in raw_dataset.iloc[:,:-3].columns :\n",
    "    for element in raw_dataset[col].unique():\n",
    "        watermelon_dataset[col+element]=np.zeros_like(raw_dataset[col])\n",
    "        watermelon_dataset.loc[raw_dataset[col]==element,col+element]=1\n",
    "for col in raw_dataset.iloc[:,-3:].columns :\n",
    "    watermelon_dataset[col]=raw_dataset[col]\n",
    "    \n",
    "print(watermelon_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0    1    2    3  4\n",
      "0    5.9  3.2  4.8  1.8  0\n",
      "1    6.9  3.1  5.4  2.1  1\n",
      "2    4.6  3.4  1.4  0.3  2\n",
      "3    4.6  3.1  1.5  0.2  2\n",
      "4    4.6  3.2  1.4  0.2  2\n",
      "5    6.7  3.1  5.6  2.4  1\n",
      "6    6.8  2.8  4.8  1.4  0\n",
      "7    7.4  2.8  6.1  1.9  1\n",
      "8    6.0  3.0  4.8  1.8  1\n",
      "9    5.0  3.5  1.3  0.3  2\n",
      "10   5.7  4.4  1.5  0.4  2\n",
      "11   6.1  2.6  5.6  1.4  1\n",
      "12   4.5  2.3  1.3  0.3  2\n",
      "13   7.7  3.0  6.1  2.3  1\n",
      "14   5.0  3.0  1.6  0.2  2\n",
      "15   4.4  3.2  1.3  0.2  2\n",
      "16   5.4  3.4  1.5  0.4  2\n",
      "17   6.7  3.3  5.7  2.5  1\n",
      "18   4.9  2.4  3.3  1.0  0\n",
      "19   6.1  3.0  4.9  1.8  1\n",
      "20   5.6  2.8  4.9  2.0  1\n",
      "21   6.4  3.2  5.3  2.3  1\n",
      "22   6.7  3.0  5.2  2.3  1\n",
      "23   6.2  2.2  4.5  1.5  0\n",
      "24   5.8  2.6  4.0  1.2  0\n",
      "25   6.4  3.2  4.5  1.5  0\n",
      "26   5.9  3.0  5.1  1.8  1\n",
      "27   5.4  3.9  1.7  0.4  2\n",
      "28   5.5  2.3  4.0  1.3  0\n",
      "29   6.3  2.5  5.0  1.9  1\n",
      "..   ...  ...  ...  ... ..\n",
      "120  6.5  2.8  4.6  1.5  0\n",
      "121  6.0  2.2  5.0  1.5  1\n",
      "122  5.2  3.4  1.4  0.2  2\n",
      "123  6.5  3.0  5.5  1.8  1\n",
      "124  6.0  2.9  4.5  1.5  0\n",
      "125  5.1  3.7  1.5  0.4  2\n",
      "126  5.0  3.3  1.4  0.2  2\n",
      "127  5.6  2.7  4.2  1.3  0\n",
      "128  5.4  3.4  1.7  0.2  2\n",
      "129  4.9  3.1  1.5  0.1  2\n",
      "130  5.7  2.6  3.5  1.0  0\n",
      "131  6.3  2.7  4.9  1.8  1\n",
      "132  5.5  4.2  1.4  0.2  2\n",
      "133  5.0  3.4  1.5  0.2  2\n",
      "134  6.0  2.2  4.0  1.0  0\n",
      "135  4.9  2.5  4.5  1.7  1\n",
      "136  6.2  2.9  4.3  1.3  0\n",
      "137  6.5  3.2  5.1  2.0  1\n",
      "138  6.5  3.0  5.8  2.2  1\n",
      "139  5.6  2.5  3.9  1.1  0\n",
      "140  5.5  2.6  4.4  1.2  0\n",
      "141  7.2  3.0  5.8  1.6  1\n",
      "142  5.0  2.0  3.5  1.0  0\n",
      "143  4.7  3.2  1.3  0.2  2\n",
      "144  5.6  3.0  4.5  1.5  0\n",
      "145  5.5  2.4  3.7  1.0  0\n",
      "146  5.0  3.4  1.6  0.4  2\n",
      "147  5.8  2.7  5.1  1.9  1\n",
      "148  5.0  2.3  3.3  1.0  0\n",
      "149  6.2  3.4  5.4  2.3  1\n",
      "\n",
      "[150 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "Iris_dataset = pd.read_table('iris.data', delimiter=',',header=None)\n",
    "Iris_dataset = Iris_dataset.sample(frac = 1).reset_index(drop=True) # shuffle\n",
    "# print(Iris_dataset[4].unique())\n",
    "# ['Iris-setosa' 'Iris-versicolor' 'Iris-virginica']\n",
    "category_dic = {}\n",
    "for i,category in enumerate(Iris_dataset[4].unique()):\n",
    "    Iris_dataset.loc[Iris_dataset[4]==category,4]=i\n",
    "    category_dic[i] = category\n",
    "print(Iris_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the class\n",
    "# about multi-classification:  https://blog.csdn.net/u013719780/article/details/78312165\n",
    "# using OVO(One-Vs-One) here\n",
    "# 均使用了OvO的多分类方法\n",
    "class LDA_Iris:\n",
    "    \n",
    "    def __init__(self,dataset):\n",
    "        self.dataset = dataset\n",
    "        \n",
    "    \n",
    "    def training(self,data_matrix_0,data_matrix_1):\n",
    "        u0 = data_matrix_0.mean(0) # [1,8]\n",
    "        u1 = data_matrix_1.mean(0)\n",
    "        \n",
    "        #def compute_sigma(self):\n",
    "        tmp0 = np.zeros_like((data_matrix_0[0,:]-u0).dot((data_matrix_0[0,:]-u0).T))\n",
    "        for raw in range(data_matrix_0.shape[0]):\n",
    "            tmp0 += (data_matrix_0[raw,:]-u0).dot((data_matrix_0[raw,:]-u0).T)\n",
    "        sigma0=1/(data_matrix_0.shape[0]-1) * tmp0\n",
    "        \n",
    "        tmp1 = np.zeros_like((data_matrix_1[0,:]-u1).dot((data_matrix_1[0,:]-u1).T))\n",
    "        for raw in range(data_matrix_1.shape[0]):\n",
    "            tmp1 += (data_matrix_1[raw,:]-u1).dot((data_matrix_1[raw,:]-u1).T)\n",
    "        sigma1=1/(data_matrix_1.shape[0]-1) * tmp1\n",
    "        \n",
    "        S_w = sigma0 + sigma1\n",
    "        \n",
    "        # def compute_w(self):\n",
    "\n",
    "        if isinstance(S_w,np.float64):\n",
    "            w = (1/S_w) * (u0-u1)\n",
    "        else:\n",
    "            w = np.linalg.inv(S_w).dot(u0-u1)\n",
    "        \n",
    "        class_0_centre =  w.T.dot(u0)\n",
    "        class_1_centre =  w.T.dot(u1)\n",
    "        \n",
    "        return w,class_0_centre,class_1_centre # [1,8]\n",
    "    \n",
    "    def  classification(self,sample,w,class_i_centre,class_j_centre,i,j):\n",
    "        \n",
    "        return i if abs(w.T.dot(sample) - class_j_centre)> \\\n",
    "                abs(w.T.dot(sample) - class_i_centre) else j\n",
    "    \n",
    "    def Cross_validation(self, k=5):\n",
    "        num_in_each_group = int(len(self.dataset)/k)\n",
    "        correct_validation = 0\n",
    "        wrong_validation = 0\n",
    "        for i in range(0,k):\n",
    "            k_train_set = pd.concat([self.dataset.iloc[0:i*num_in_each_group,:],  \\\n",
    "                                self.dataset.iloc[(i+1)*num_in_each_group:,:]])\n",
    "\n",
    "            k_class_0_set = (k_train_set[k_train_set[4]==0].values)[:,:-1]\n",
    "            k_class_1_set = (k_train_set[k_train_set[4]==1].values)[:,:-1]\n",
    "            k_class_2_set = (k_train_set[k_train_set[4]==2].values)[:,:-1]\n",
    "\n",
    "            k_val_set = self.dataset.iloc[i*num_in_each_group:(i+1)*num_in_each_group,:]\n",
    "\n",
    "            # using OVO \n",
    "            w_01,class_0_centre_01,class_1_centre_01 = self.training(k_class_0_set,k_class_1_set)\n",
    "            w_02,class_0_centre_02,class_2_centre_02 = self.training(k_class_0_set,k_class_2_set)\n",
    "            w_12,class_1_centre_12,class_2_centre_12 = self.training(k_class_1_set,k_class_2_set)\n",
    "            \n",
    "            \n",
    "            for j in range(len(k_val_set.index)):\n",
    "                vector_x = k_val_set.iloc[j,:-1].values # [1,8]\n",
    "                groud_truth = k_val_set.iloc[j,-1]\n",
    "\n",
    "                predict_value_01 = self.classification(vector_x,w_01,class_0_centre_01,class_1_centre_01,0,1)\n",
    "                predict_value_02 = self.classification(vector_x,w_02,class_0_centre_02,class_2_centre_02,0,2)\n",
    "                predict_value_12 = self.classification(vector_x,w_12,class_1_centre_12,class_2_centre_12,1,2)\n",
    "                \n",
    "                if predict_value_01 == predict_value_02:\n",
    "                    predict_value = 0\n",
    "                elif predict_value_01 == predict_value_12:\n",
    "                    predict_value = 1\n",
    "                elif predict_value_12 == predict_value_02:\n",
    "                    predict_value = 2\n",
    "                else:\n",
    "                    predict_value = np.random.choice([0,1,2])\n",
    "                \n",
    "                \n",
    "                if groud_truth==predict_value:\n",
    "                    correct_validation+=1\n",
    "                else:\n",
    "                    wrong_validation+=1\n",
    "        return float(correct_validation/(wrong_validation+correct_validation))\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDA_watermelon:\n",
    "    def __init__(self,dataset):\n",
    "        self.dataset = dataset\n",
    "        \n",
    "    def training(self,data_matrix_0,data_matrix_1):\n",
    "        u0 = data_matrix_0.mean(0) # [1,8]\n",
    "        u1 = data_matrix_1.mean(0)\n",
    "        \n",
    "        #def compute_sigma(self):\n",
    "        tmp0 = np.zeros_like((data_matrix_0[0,:]-u0).dot((data_matrix_0[0,:]-u0).T))\n",
    "        for raw in range(data_matrix_0.shape[0]):\n",
    "            tmp0 += (data_matrix_0[raw,:]-u0).dot((data_matrix_0[raw,:]-u0).T)\n",
    "        sigma0=1/(data_matrix_0.shape[0]-1) * tmp0\n",
    "        \n",
    "        tmp1 = np.zeros_like((data_matrix_1[0,:]-u1).dot((data_matrix_1[0,:]-u1).T))\n",
    "        for raw in range(data_matrix_1.shape[0]):\n",
    "            tmp1 += (data_matrix_1[raw,:]-u1).dot((data_matrix_1[raw,:]-u1).T)\n",
    "        sigma1=1/(data_matrix_1.shape[0]-1) * tmp1\n",
    "        \n",
    "        S_w = sigma0 + sigma1\n",
    "        \n",
    "        # def compute_w(self):\n",
    "\n",
    "        if isinstance(S_w,np.float64):\n",
    "            w = (1/S_w) * (u0-u1)\n",
    "        else:\n",
    "            w = np.linalg.inv(S_w).dot(u0-u1)\n",
    "        \n",
    "        class_0_centre =  w.T.dot(u0)\n",
    "        class_1_centre =  w.T.dot(u1)\n",
    "        \n",
    "        return w,class_0_centre,class_1_centre # [1,8]\n",
    "    \n",
    "    def  classification(self,sample,w,class_0_centre,class_1_centre):\n",
    "        \n",
    "        return 1 if abs(w.T.dot(sample) - class_0_centre)> \\\n",
    "                abs(w.T.dot(sample) - class_1_centre) else 0\n",
    "    \n",
    "    def Cross_validation(self, k=5):\n",
    "        num_in_each_group = int(len(self.dataset)/k)\n",
    "        correct_validation = 0\n",
    "        wrong_validation = 0\n",
    "        for i in range(0,k):\n",
    "            k_train_set = pd.concat([self.dataset.iloc[0:i*num_in_each_group,:],  \\\n",
    "                                self.dataset.iloc[(i+1)*num_in_each_group:,:]])\n",
    "\n",
    "            k_class_0_set = (k_train_set[k_train_set['好瓜']==0].values)[:,:-1]\n",
    "            k_class_1_set = (k_train_set[k_train_set['好瓜']==1].values)[:,:-1]\n",
    "\n",
    "            k_val_set = self.dataset.iloc[i*num_in_each_group:(i+1)*num_in_each_group,:]\n",
    "\n",
    "\n",
    "            w,class_0_centre,class_1_centre = self.training(k_class_0_set,k_class_1_set)\n",
    "            \n",
    "            \n",
    "            for j in range(len(k_val_set.index)):\n",
    "                vector_x = k_val_set.iloc[j,:-1].values # [1,8]\n",
    "                groud_truth = k_val_set.iloc[j,-1]\n",
    "\n",
    "                predict_value = self.classification(vector_x,w,class_0_centre,class_1_centre)\n",
    "                if groud_truth==predict_value:\n",
    "                    correct_validation+=1\n",
    "                else:\n",
    "                    wrong_validation+=1\n",
    "        return float(correct_validation/(wrong_validation+correct_validation))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the class\n",
    "class Naive_Bayes_watermelon:\n",
    "    \n",
    "    def __init__(self,dataset):\n",
    "        self.dataset = dataset\n",
    "            \n",
    "        \n",
    "    def MLE(self,sample_density,sample_sugar_content,class_i_dataset):\n",
    "        def compute_probability(x,mu,sigma):\n",
    "            exp = math.exp(-(x-mu)**2/(2*(sigma**2)))\n",
    "            coe = 1/(((2*math.pi)**0.5)*sigma)\n",
    "            return coe*exp\n",
    "        density = class_i_dataset[:,-3] # [N,1]\n",
    "        sugar_content = class_i_dataset[:,-2] # [N,1]\n",
    "        \n",
    "        mu_d = density.mean()\n",
    "        mu_s = sugar_content.mean()\n",
    "        \n",
    "        sigma_d = np.std(density)\n",
    "        sigma_s = np.std(sugar_content)\n",
    "        \n",
    "        return (compute_probability(sample_density, mu_d, sigma_d),\n",
    "                        compute_probability(sample_sugar_content, mu_s, sigma_s) )\n",
    "        \n",
    "    def classification(self,sample_property,train_classification,class_0_set,class_1_set):\n",
    "\n",
    "        num_of_train_set = len(train_classification)\n",
    "        num_of_class_0 = len(train_classification[train_classification==0])\n",
    "        class_0_probability = 1*num_of_class_0/num_of_train_set\n",
    "        for i in range(len(sample_property)-2):\n",
    "            class_0_probability *= len(class_0_set[class_0_set[:,i]==sample_property[i]]) \\\n",
    "                                    /num_of_class_0\n",
    "        \n",
    "        p_den_0,p_sug_0 = self.MLE(sample_property[-2],sample_property[-1],class_0_set)\n",
    "        class_0_probability = class_0_probability*p_den_0*p_sug_0\n",
    "        \n",
    "        num_of_class_1 = len(train_classification[train_classification==1])\n",
    "        class_1_probability = 1*num_of_class_1/num_of_train_set\n",
    "        for i in range(len(sample_property)-2):\n",
    "            class_1_probability *= len(class_1_set[class_1_set[:,i]==sample_property[i]]) \\\n",
    "                                    /num_of_class_1\n",
    "        p_den_1,p_sug_1 = self.MLE(sample_property[-2],sample_property[-1],class_1_set)\n",
    "        class_1_probability = class_1_probability*p_den_1*p_sug_1\n",
    "        \n",
    "        return (1 if class_0_probability<class_1_probability else 0)   \n",
    "\n",
    "    def Cross_validation(self, k=5):\n",
    "        num_in_each_group = int(len(self.dataset)/k)\n",
    "        correct_classification = 0\n",
    "        wrong_classification = 0\n",
    "\n",
    "        for i in range(0,k):\n",
    "            \n",
    "            k_train_set = pd.concat([self.dataset.iloc[0:i*num_in_each_group,:],  \\\n",
    "                                self.dataset.iloc[(i+1)*num_in_each_group:,:]])\n",
    "            k_train_classification =  k_train_set.iloc[:,-1].values\n",
    "            k_class_0_set = k_train_set[k_train_set['好瓜']==0].values \n",
    "            k_class_1_set = k_train_set[k_train_set['好瓜']==1].values \n",
    "\n",
    "            k_val_set = self.dataset.iloc[i*num_in_each_group:(i+1)*num_in_each_group,:]\n",
    "            k_val_property = k_val_set.iloc[:,:-1].values\n",
    "\n",
    "            for j in range((k_val_property.shape[0])) :\n",
    "                if(k_val_set.iloc[j,-1]==   \\\n",
    "                        self.classification(k_val_property[j,:],k_train_classification,k_class_0_set,k_class_1_set)):\n",
    "                    correct_classification += 1\n",
    "                else :\n",
    "                    wrong_classification += 1\n",
    "        # print(correct_classification,wrong_classification)\n",
    "        return correct_classification/(correct_classification+wrong_classification)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the class\n",
    "class Naive_Bayes_Iris:\n",
    "    \n",
    "    def __init__(self,dataset):\n",
    "        self.dataset = dataset\n",
    "            \n",
    "        \n",
    "    def MLE(self,sample_character,i,class_i_dataset):\n",
    "        def compute_probability(x,mu,sigma):\n",
    "            exp = math.exp(-(x-mu)**2/(2*(sigma**2)))\n",
    "            coe = 1/(((2*math.pi)**0.5)*sigma)\n",
    "            return coe*exp\n",
    "        set_character = class_i_dataset[:,i] # [N,1]\n",
    "        \n",
    "        mu= set_character.mean()\n",
    "        \n",
    "        sigma= np.std(set_character)\n",
    "\n",
    "        \n",
    "        return compute_probability(sample_character, mu, sigma)\n",
    "                        \n",
    "        \n",
    "    def classification(self,sample_property,train_classification,class_0_set,class_1_set,class_2_set):\n",
    "\n",
    "        num_of_train_set = len(train_classification)\n",
    "        num_of_class_0 = len(train_classification[train_classification==0])\n",
    "        class_0_probability = 1*num_of_class_0/num_of_train_set\n",
    "        for i in range(len(sample_property)):\n",
    "            class_0_probability *= self.MLE(sample_property[i],i,class_0_set)\n",
    "            # print(i)\n",
    "        #-----------------------------------------------------------------------\n",
    "        num_of_class_1 = len(train_classification[train_classification==1])\n",
    "        class_1_probability = 1*num_of_class_1/num_of_train_set\n",
    "        for i in range(len(sample_property)):\n",
    "            class_1_probability *= self.MLE(sample_property[i],i,class_1_set)\n",
    "\n",
    "        #-----------------------------------------------------------------------\n",
    "        num_of_class_2 = len(train_classification[train_classification==2])\n",
    "        class_2_probability = 1*num_of_class_2/num_of_train_set\n",
    "        for i in range(len(sample_property)):\n",
    "            class_2_probability *= self.MLE(sample_property[i],i,class_2_set)\n",
    "        \n",
    "\n",
    "        if class_2_probability == max(class_2_probability,class_1_probability,class_0_probability):\n",
    "            return 2\n",
    "        elif class_1_probability == max(class_2_probability,class_1_probability,class_0_probability):\n",
    "            return 1\n",
    "        else :\n",
    "            return 0\n",
    "\n",
    "    def Cross_validation(self, k=5):\n",
    "        num_in_each_group = int(len(self.dataset)/k)\n",
    "        correct_classification = 0\n",
    "        wrong_classification = 0\n",
    "\n",
    "        for i in range(0,k):\n",
    "            \n",
    "            k_train_set = pd.concat([self.dataset.iloc[0:i*num_in_each_group,:],  \\\n",
    "                                self.dataset.iloc[(i+1)*num_in_each_group:,:]])\n",
    "            k_train_classification =  k_train_set.iloc[:,-1].values\n",
    "            k_class_0_set = k_train_set[k_train_set[4]==0].values \n",
    "            k_class_1_set = k_train_set[k_train_set[4]==1].values \n",
    "            k_class_2_set = k_train_set[k_train_set[4]==2].values \n",
    "\n",
    "            k_val_set = self.dataset.iloc[i*num_in_each_group:(i+1)*num_in_each_group,:]\n",
    "            k_val_property = k_val_set.iloc[:,:-1].values\n",
    "\n",
    "            for j in range((k_val_property.shape[0])) :\n",
    "                if(k_val_set.iloc[j,-1]==   \\\n",
    "                        self.classification(k_val_property[j,:],k_train_classification,k_class_0_set,k_class_1_set,k_class_2_set)):\n",
    "                    correct_classification += 1\n",
    "                else :\n",
    "                    wrong_classification += 1\n",
    "        # print(correct_classification,wrong_classification)\n",
    "        return correct_classification/(correct_classification+wrong_classification)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM_method:\n",
    "    def __init__(self,dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "        \n",
    "    def train(self,train_matrix,train_classification,kernel=None):\n",
    "        model = svm.SVC(kernel=kernel)\n",
    "        model.fit(X=train_matrix, y=train_classification,sample_weight=None)\n",
    "        return model\n",
    "                \n",
    "        \n",
    "    def cernel_comparasion(self):\n",
    "        kernel_list = ['linear','poly','rbf','sigmoid']\n",
    "        for kernel_ in kernel_list:\n",
    "            self.Cross_validation(kernel_)\n",
    "    \n",
    "    def Cross_validation(self, kernel, k=5):\n",
    "        num_in_each_group = int(len(self.dataset)/k)\n",
    "        correct_classification = 0\n",
    "        wrong_classification = 0\n",
    "\n",
    "        for i in range(0,k):\n",
    "            \n",
    "            k_train_set = pd.concat([self.dataset.iloc[0:i*num_in_each_group,:],  \\\n",
    "                                self.dataset.iloc[(i+1)*num_in_each_group:,:]])\n",
    "            k_train_classification =  k_train_set.iloc[:,-1].values\n",
    "            k_train_set = k_train_set.iloc[:,:-1].values\n",
    "            \n",
    "            k_val_set = self.dataset.iloc[i*num_in_each_group:(i+1)*num_in_each_group,:]\n",
    "            k_val_property = k_val_set.iloc[:,:-1].values\n",
    "\n",
    "            model = self.train(k_train_set, k_train_classification, kernel)\n",
    "\n",
    "            for j in range((k_val_property.shape[0])) :\n",
    "\n",
    "                if(k_val_set.iloc[j,-1]==   \\\n",
    "                         model.predict(k_val_property[None,j,:])):\n",
    "                    correct_classification += 1\n",
    "                else :\n",
    "                    wrong_classification += 1\n",
    "        # print(correct_classification,wrong_classification)\n",
    "        accuracy = correct_classification/(correct_classification+wrong_classification)\n",
    "        print(('Accuracy with Kernel:'+ kernel+':',\"%.2f%%\"  %(accuracy*100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logistic_Regression_Iris:\n",
    "    def __init__(self,dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def Cross_validation(self, k=5):\n",
    "        num_in_each_group = int(len(self.dataset)/k)\n",
    "        correct_classification = 0\n",
    "        wrong_classification = 0\n",
    "\n",
    "        for i in range(0,k):\n",
    "            \n",
    "            k_train_set = pd.concat([self.dataset.iloc[0:i*num_in_each_group,:],  \\\n",
    "                                self.dataset.iloc[(i+1)*num_in_each_group:,:]])\n",
    "            k_class_0_set = (k_train_set[k_train_set[4]==0].values)[:,:-1]\n",
    "            k_class_1_set = (k_train_set[k_train_set[4]==1].values)[:,:-1]\n",
    "            k_class_2_set = (k_train_set[k_train_set[4]==2].values)[:,:-1]\n",
    "\n",
    "            k_val_set = self.dataset.iloc[i*num_in_each_group:(i+1)*num_in_each_group,:]\n",
    "            k_val_property = k_val_set.iloc[:,:-1].values\n",
    "\n",
    "            k_w01, k_b01 = self.Gradient_Descent(k_class_1_set,k_class_0_set) #1->1, 0->0\n",
    "            k_w02, k_b02 = self.Gradient_Descent(k_class_2_set,k_class_0_set) #2->1, 0->0\n",
    "            k_w12, k_b12 = self.Gradient_Descent(k_class_1_set,k_class_2_set) #1->1, 2->0\n",
    "\n",
    "            for j in range((k_val_property.shape[0])) :\n",
    "\n",
    "                if(k_val_set.iloc[j,-1]==   \\\n",
    "                        self.LR_OvO(k_val_property[j,:], k_w01, k_b01, k_w02, k_b02, k_w12, k_b12)):\n",
    "                    correct_classification += 1\n",
    "                else :\n",
    "                    wrong_classification += 1\n",
    "        # print(correct_classification,wrong_classification)\n",
    "        return correct_classification/(correct_classification+wrong_classification)\n",
    "    \n",
    "    def Gradient_Descent(self,class_1_set,class_0_set,learning_rate = 0.1):\n",
    "        def p_y_is_1(w,x,b):\n",
    "            tmp = math.exp(w.dot(x)+b)\n",
    "            # print(tmp)\n",
    "            return tmp/(1+tmp)\n",
    "        w = np.ones_like(class_1_set[0])\n",
    "        b = 1\n",
    "        prev_w = w\n",
    "        prev_b = b\n",
    "\n",
    "        while True:\n",
    "            prev_w = w\n",
    "            prev_b = b\n",
    "            sigma_1 = 0\n",
    "            sigma_0 = 0\n",
    "            for i in range(class_1_set.shape[0]):\n",
    "                sigma_1 += class_1_set[i] * p_y_is_1(prev_w, class_1_set[i], prev_b)\n",
    "            for i in range(class_0_set.shape[0]):\n",
    "                sigma_0 += class_0_set[i] * p_y_is_1(prev_w, class_0_set[i], prev_b)\n",
    "            w = prev_w + learning_rate * (class_1_set.sum(0) -sigma_1-sigma_0 )\n",
    "\n",
    "            sigma_1 = 0\n",
    "            sigma_0 = 0\n",
    "            for i in range(class_1_set.shape[0]):\n",
    "                sigma_1 += p_y_is_1(prev_w, class_1_set[i], prev_b)\n",
    "            for i in range(class_0_set.shape[0]):\n",
    "                sigma_0 += p_y_is_1(prev_w, class_0_set[i], prev_b)\n",
    "            b = prev_b + learning_rate * (class_1_set.shape[0] -sigma_1-sigma_0 )\n",
    "            if abs(sum(prev_w-w))<1e-2:\n",
    "                break\n",
    "\n",
    "        return w,b\n",
    "    \n",
    "    def LR_OvO(self, val_property, k_w01, k_b01, k_w02, k_b02, k_w12, k_b12):\n",
    "\n",
    "        compare01 =  (1 if 1/(math.exp(k_w01.dot(val_property)+k_b01)+1)<0.5 else 0)\n",
    "        compare02 =  (2 if 1/(math.exp(k_w02.dot(val_property)+k_b02)+1)<0.5 else 0)\n",
    "        compare12 =  (1 if 1/(math.exp(k_w12.dot(val_property)+k_b12)+1)<0.5 else 2)\n",
    "\n",
    "        if compare01==compare02:\n",
    "            return 0\n",
    "        elif compare01==compare12:\n",
    "            return 1\n",
    "        elif compare02==compare12:\n",
    "            return 2\n",
    "        else :\n",
    "            return np.random.choice([0,1,2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logistic_Regression_watermelon:\n",
    "    def __init__(self,dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def Cross_validation(self, k=5):\n",
    "        num_in_each_group = int(len(self.dataset)/k)\n",
    "        correct_classification = 0\n",
    "        wrong_classification = 0\n",
    "\n",
    "        for i in range(0,k):\n",
    "            \n",
    "            k_train_set = pd.concat([self.dataset.iloc[0:i*num_in_each_group,:],  \\\n",
    "                                self.dataset.iloc[(i+1)*num_in_each_group:,:]])\n",
    "            k_class_0_set = (k_train_set[k_train_set['好瓜']==0].values)[:,:-1]\n",
    "            k_class_1_set = (k_train_set[k_train_set['好瓜']==1].values)[:,:-1]\n",
    "\n",
    "            k_val_set = self.dataset.iloc[i*num_in_each_group:(i+1)*num_in_each_group,:]\n",
    "            k_val_property = k_val_set.iloc[:,:-1].values\n",
    "\n",
    "            k_w01, k_b01 = self.Gradient_Descent(k_class_1_set,k_class_0_set) #1->1, 0->0\n",
    "\n",
    "            for j in range((k_val_property.shape[0])) :\n",
    "\n",
    "                if(k_val_set.iloc[j,-1]==   \\\n",
    "                        (1 if 1/(math.exp(k_w01.dot(k_val_property[j,:])+k_b01)+1)<0.5 else 0)):\n",
    "                    correct_classification += 1\n",
    "                else :\n",
    "                    wrong_classification += 1\n",
    "        # print(correct_classification,wrong_classification)\n",
    "        return correct_classification/(correct_classification+wrong_classification)\n",
    "    \n",
    "    def Gradient_Descent(self,class_1_set,class_0_set,learning_rate = 0.1):\n",
    "        def p_y_is_1(w,x,b):\n",
    "            tmp = math.exp(w.dot(x)+b)\n",
    "            # print(tmp)\n",
    "            return tmp/(1+tmp)\n",
    "        w = np.ones_like(class_1_set[0])\n",
    "        b = 1\n",
    "        prev_w = w\n",
    "        prev_b = b\n",
    "\n",
    "        while True:\n",
    "            prev_w = w\n",
    "            prev_b = b\n",
    "            sigma_1 = 0\n",
    "            sigma_0 = 0\n",
    "            for i in range(class_1_set.shape[0]):\n",
    "                sigma_1 += class_1_set[i] * p_y_is_1(prev_w, class_1_set[i], prev_b)\n",
    "            for i in range(class_0_set.shape[0]):\n",
    "                sigma_0 += class_0_set[i] * p_y_is_1(prev_w, class_0_set[i], prev_b)\n",
    "            w = prev_w + learning_rate * (class_1_set.sum(0) -sigma_1-sigma_0 )\n",
    "\n",
    "            sigma_1 = 0\n",
    "            sigma_0 = 0\n",
    "            for i in range(class_1_set.shape[0]):\n",
    "                sigma_1 += p_y_is_1(prev_w, class_1_set[i], prev_b)\n",
    "            for i in range(class_0_set.shape[0]):\n",
    "                sigma_0 += p_y_is_1(prev_w, class_0_set[i], prev_b)\n",
    "            b = prev_b + learning_rate * (class_1_set.shape[0] -sigma_1-sigma_0 )\n",
    "            if abs(sum(prev_w-w))<1e-2:\n",
    "                break\n",
    "\n",
    "        return w,b\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using LDA method:\n",
      "Accuracy on Watermelon validation set: 73.33%\n",
      "Accuracy on Iris validation set: 90.67%\n",
      "\n",
      "Using Naive Bayes method:\n",
      "Accuracy on Watermelon validation set: 66.67%\n",
      "Accuracy on Iris validation set: 94.00%\n",
      "\n",
      "Using SVM method:\n",
      "On watermelon dataset:\n",
      "('Accuracy with Kernel:linear:', '73.33%')\n",
      "('Accuracy with Kernel:poly:', '60.00%')\n",
      "('Accuracy with Kernel:rbf:', '66.67%')\n",
      "('Accuracy with Kernel:sigmoid:', '60.00%')\n",
      "On Iris dataset:\n",
      "('Accuracy with Kernel:linear:', '98.67%')\n",
      "('Accuracy with Kernel:poly:', '97.33%')\n",
      "('Accuracy with Kernel:rbf:', '98.00%')\n",
      "('Accuracy with Kernel:sigmoid:', '22.67%')\n",
      "\n",
      "Using Logistic Regression method:\n",
      "Accuracy on Watermelon validation set: 66.67%\n",
      "Accuracy on Iris validation set: 97.33%\n"
     ]
    }
   ],
   "source": [
    "# with LDA method:\n",
    "print('Using LDA method:')\n",
    "LDA_watermelon_classification = LDA_watermelon(watermelon_dataset)\n",
    "print('Accuracy on Watermelon validation set:',\"%.2f%%\"  %(LDA_watermelon_classification.Cross_validation()*100))\n",
    "LDA_Iris_classification = LDA_Iris(Iris_dataset)\n",
    "print('Accuracy on Iris validation set:',\"%.2f%%\"  %(LDA_Iris_classification.Cross_validation()*100))\n",
    "\n",
    "# with Bayes method:\n",
    "print('\\nUsing Naive Bayes method:')\n",
    "Naive_Bayes_watermelon_classification = Naive_Bayes_watermelon(watermelon_dataset)\n",
    "print('Accuracy on Watermelon validation set:',\"%.2f%%\"  %(Naive_Bayes_watermelon_classification.Cross_validation()*100))\n",
    "Naive_Bayes_Iris_classification = Naive_Bayes_Iris(Iris_dataset)\n",
    "print('Accuracy on Iris validation set:',\"%.2f%%\"  %(Naive_Bayes_Iris_classification.Cross_validation()*100))\n",
    "\n",
    "# with SVM method:\n",
    "print('\\nUsing SVM method:')\n",
    "print('On watermelon dataset:')\n",
    "SVM_watermelon_classification = SVM_method(watermelon_dataset)\n",
    "SVM_watermelon_classification.cernel_comparasion( )\n",
    "print('On Iris dataset:')\n",
    "SVM_Iris_classification = SVM_method(Iris_dataset)\n",
    "SVM_Iris_classification.cernel_comparasion()\n",
    "\n",
    "# with Logistic Regression method:\n",
    "print('\\nUsing Logistic Regression method:')\n",
    "Logistic_Regression_watermelon_classification = Logistic_Regression_watermelon(watermelon_dataset)\n",
    "print('Accuracy on Watermelon validation set:',\"%.2f%%\"  %(Logistic_Regression_watermelon_classification.Cross_validation()*100))\n",
    "Logistic_Regression_Iris_classification = Logistic_Regression_Iris(Iris_dataset)\n",
    "print('Accuracy on Iris validation set:',\"%.2f%%\"  %(Logistic_Regression_Iris_classification.Cross_validation()*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
